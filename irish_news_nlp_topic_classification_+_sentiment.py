# -*- coding: utf-8 -*-
"""Irish News NLP: Topic Classification + Sentiment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUGtMdeMCN2XR81Ge6233pl-uEGv35GQ
"""

# Step 1: Install and import the libraries we need

!pip install nltk --quiet

import os
import zipfile

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    f1_score
)

import nltk
import re
import string

from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer

# download NLTK resources
nltk.download('stopwords')
nltk.download('vader_lexicon')

sns.set_theme()

STOPWORDS = set(stopwords.words("english"))
sia = SentimentIntensityAnalyzer()

# Step 2: Extract the zip file with all the text files
# Make sure you uploaded "IrishTimes_News_Dataset.zip" to Colab first

ZIP_NAME = "IrishTimes_News_Dataset.zip"
zip_path = os.path.join("/content", ZIP_NAME)

if os.path.exists(zip_path):
    with zipfile.ZipFile(zip_path, "r") as z:
        z.extractall("/content/")
    print("Zip extracted.")
else:
    print("Zip file not found. Please upload IrishTimes_News_Dataset.zip first.")

# quick check of files in /content
os.listdir("/content")

# Step 3: Set the paths for the train/test text and label files

train_txt_path = "/content/new_IrishTimes_train.txt"
train_label_path = "/content/new_IrishTimes_train_label.txt"
test_txt_path = "/content/new_IrishTimes_test.txt"
test_label_path = "/content/new_IrishTimes_test_label.txt"

for path in [train_txt_path, train_label_path, test_txt_path, test_label_path]:
    print(path, "exists:", os.path.exists(path))

# Step 4: Helper functions for reading and cleaning text

def read_lines(path, encoding="utf-8"):
    with open(path, "r", encoding=encoding, errors="ignore") as f:
        lines = [line.strip() for line in f.readlines()]
    return lines

def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r"http\S+|www\.\S+", "", text)              # remove links
    text = re.sub(r"\d+", "", text)                           # remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # remove punctuation
    tokens = text.split()
    tokens = [w for w in tokens if w not in STOPWORDS]        # remove stopwords
    return " ".join(tokens)

# Step 5: Read the raw text and labels into DataFrames

X_train_raw = read_lines(train_txt_path)
y_train_raw = read_lines(train_label_path)

X_test_raw = read_lines(test_txt_path)
y_test_raw = read_lines(test_label_path)

print("Train samples:", len(X_train_raw), "labels:", len(y_train_raw))
print("Test samples:", len(X_test_raw), "labels:", len(y_test_raw))

# basic sanity check
assert len(X_train_raw) == len(y_train_raw), "train text and label length mismatch"
assert len(X_test_raw) == len(y_test_raw), "test text and label length mismatch"

train_df = pd.DataFrame({"text": X_train_raw, "label": y_train_raw})
test_df = pd.DataFrame({"text": X_test_raw, "label": y_test_raw})

# create a cleaned version for modelling
train_df["clean_text"] = train_df["text"].astype(str).apply(clean_text)
test_df["clean_text"] = test_df["text"].astype(str).apply(clean_text)

train_df.head()

# Step 6: Quick EDA on article length (just to see the distribution)

train_df["length"] = train_df["text"].str.len()

plt.figure(figsize=(8, 4))
train_df["length"].hist(bins=40, color="skyblue")
plt.title("Distribution of Article Length (Training Set)")
plt.xlabel("Character Count")
plt.ylabel("Frequency")
plt.show()

# Step 7: Look at the label distribution in the training set

plt.figure(figsize=(10, 4))
train_df["label"].value_counts().plot(kind="bar")
plt.title("Training Label Distribution")
plt.xticks(rotation=40)
plt.show()

print("Unique labels:", sorted(train_df["label"].unique()))

# Step 8: TF-IDF feature extraction and train two models
# (Linear SVM + Logistic Regression)

tfidf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1, 2),
    min_df=2
)

X_train_vec = tfidf.fit_transform(train_df["clean_text"])
X_test_vec = tfidf.transform(test_df["clean_text"])

y_train = train_df["label"].values
y_test = test_df["label"].values

print("Train vector shape:", X_train_vec.shape)
print("Test vector shape:", X_test_vec.shape)

# Model 1: Linear SVM
svm_model = LinearSVC()
svm_model.fit(X_train_vec, y_train)
y_pred_svm = svm_model.predict(X_test_vec)

print("=== Linear SVM ===")
print(classification_report(y_test, y_pred_svm))

cm_svm = confusion_matrix(y_test, y_pred_svm, labels=svm_model.classes_)
disp = ConfusionMatrixDisplay(cm_svm, display_labels=svm_model.classes_)
disp.plot(cmap="Blues", xticks_rotation=45)
plt.title("Linear SVM - Confusion Matrix")
plt.show()

# Model 2: Logistic Regression
log_model = LogisticRegression(max_iter=1000, n_jobs=-1, random_state=42)
log_model.fit(X_train_vec, y_train)
y_pred_log = log_model.predict(X_test_vec)

print("=== Logistic Regression ===")
print(classification_report(y_test, y_pred_log))

cm_log = confusion_matrix(y_test, y_pred_log, labels=log_model.classes_)
disp2 = ConfusionMatrixDisplay(cm_log, display_labels=log_model.classes_)
disp2.plot(cmap="Oranges", xticks_rotation=45)
plt.title("Logistic Regression - Confusion Matrix")
plt.show()

# Step 9: Compare both models using accuracy and weighted F1 score

svm_acc = accuracy_score(y_test, y_pred_svm)
log_acc = accuracy_score(y_test, y_pred_log)

svm_f1 = f1_score(y_test, y_pred_svm, average="weighted")
log_f1 = f1_score(y_test, y_pred_log, average="weighted")

print("Model Comparison Summary:")
print(f"SVM  -> Accuracy: {svm_acc:.3f}, Weighted F1: {svm_f1:.3f}")
print(f"LOG  -> Accuracy: {log_acc:.3f}, Weighted F1: {log_f1:.3f}")

# Step 10: Small helper to predict the topic of a new sentence

def predict_topic(text: str) -> str:
    cleaned = clean_text(text)
    vec = tfidf.transform([cleaned])
    pred = svm_model.predict(vec)[0]
    return pred

sample_sentences = [
    "Ireland launches new AI startup program in Dublin",
    "Hospitals are facing increased pressure after winter flu season",
    "Irish football team secures a big win in the final",
]

for s in sample_sentences:
    print(s, "->", predict_topic(s))

# Step 11: Add simple sentiment labels (positive / neutral / negative) using VADER

def vader_sentiment_label(text: str) -> str:
    score = sia.polarity_scores(text)["compound"]
    if score >= 0.05:
        return "positive"
    elif score <= -0.05:
        return "negative"
    else:
        return "neutral"

train_df["sentiment"] = train_df["text"].apply(vader_sentiment_label)
test_df["sentiment"] = test_df["text"].apply(vader_sentiment_label)

combined_df = pd.concat(
    [train_df.assign(split="train"), test_df.assign(split="test")],
    ignore_index=True
)

combined_df[["text", "label", "sentiment"]].head()

# Step 12: Plot sentiment distribution for each news topic

plt.figure(figsize=(10, 5))
sns.countplot(data=combined_df, x="label", hue="sentiment")
plt.title("Sentiment Distribution by Topic")
plt.xticks(rotation=40)
plt.tight_layout()
plt.show()

# overall sentiment distribution
plt.figure(figsize=(5, 4))
combined_df["sentiment"].value_counts().plot(kind="bar", color="coral")
plt.title("Overall Sentiment Count")
plt.ylabel("Count")
plt.show()

# Step 13 (optional): Look at some common words for each topic

from collections import Counter

def top_words_for_label(df, label_name, top_n=15):
    subset = df.loc[df["label"] == label_name, "clean_text"].str.split()
    all_tokens = []
    for tokens in subset:
        all_tokens.extend(tokens)
    counter = Counter(all_tokens)
    return counter.most_common(top_n)

for lbl in sorted(combined_df["label"].unique()):
    print("\nTop words for label:", lbl)
    print(top_words_for_label(combined_df, lbl, top_n=10))